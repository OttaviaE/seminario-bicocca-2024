# Test length doesn’t matter (too much): Item response theory procedures for shortening tests
## Seminar @Milano-Bicocca, November 2024
 
Given the information on the measurement precision of each item with respect to
different levels of the latent trait, item response theory provides an ideal framework
for shortening tests. This contribution presents four procedures for identifying the
best items to include in a short version (SV) of a test. The procedures differ
according to how the information on the latent trait is considered for the item
inclusion in the SV (i.e., either based on the definition of discrete levels of interest –
denoted as theta targets – or based on the peculiarities of the continuous latent
variable that the SV should recreate – denoted as target test information) and
according to the item selection methods used. While the procedure based on the
definition of the theta targets selects the items that maximize the information with
respect to the defined theta target levels, the one based on the definition of the
target test information aims at minimizing the distance between the test information
provided by the SV and the target test information. In the latter case, different item
selection methods are taken into account, and their performance is compared
against the best SV identified through a brute force method (i.e., systematic test of
all SVs obtainable from an item bank). The (promising) results of a simulation study
are presented. Questions remain open, like: are these approaches useful on real
data?

[Slides](irt-stf.html)